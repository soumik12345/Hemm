{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hemm: Holistic Evaluation of Multi-modal Generative Models","text":"<p>Hemm is a library for performing comprehensive benchmark of text-to-image diffusion models on image quality and prompt comprehension integrated with Weights &amp; Biases and Weave. Hemm is inspired by Holistic Evaluation of Text-To-Image Models.</p> <p>Warning</p> <p>The ownership of Hemm has been turned over to the Weights &amp; Biases organization. All development will henceforth continue on https://github.com/wandb/Hemm and docs will be visible on https://wandb.github.io/Hemm/ and https://github.com/soumik12345/Hemm is hereby archived.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/soumik12345/Hemm\ncd Hemm\npip install -e \".[core]\"\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>First let's publish a small subset of the MSCOCO validation set as a Weave Dataset.</p> <pre><code>import weave\nfrom hemm.utils import publish_dataset_to_weave\n\nweave.init(project_name=\"t2i_eval\")\n\ndataset_reference = publish_dataset_to_weave(\n    dataset_path=\"HuggingFaceM4/COCO\",\n    prompt_column=\"sentences\",\n    ground_truth_image_column=\"image\",\n    split=\"validation\",\n    dataset_transforms=[\n        lambda item: {**item, \"sentences\": item[\"sentences\"][\"raw\"]}\n    ],\n    data_limit=5,\n)\n</code></pre> Weave Datasets enable you to collect examples for evaluation and automatically track versions for accurate comparisons. Easily update datasets with the UI and download the latest version locally with a simple API. <p>Next, you can evaluate Stable Diffusion 1.4 on image quality metrics as shown in the following code snippet:</p> <pre><code>import wandb\nimport weave\n\nfrom hemm.eval_pipelines import BaseWeaveModel, EvaluationPipeline\nfrom hemm.metrics.image_quality import LPIPSMetric, PSNRMetric, SSIMMetric\n\n# Initialize Weave and WandB\nwandb.init(project=\"image-quality-leaderboard\", job_type=\"evaluation\")\nweave.init(project_name=\"image-quality-leaderboard\")\n\n# Initialize the diffusion model to be evaluated as a `weave.Model` using `BaseWeaveModel`\nmodel = BaseWeaveModel(diffusion_model_name_or_path=\"CompVis/stable-diffusion-v1-4\")\n\n# Add the model to the evaluation pipeline\nevaluation_pipeline = EvaluationPipeline(model=model)\n\n# Add PSNR Metric to the evaluation pipeline\npsnr_metric = PSNRMetric(image_size=evaluation_pipeline.image_size)\nevaluation_pipeline.add_metric(psnr_metric)\n\n# Add SSIM Metric to the evaluation pipeline\nssim_metric = SSIMMetric(image_size=evaluation_pipeline.image_size)\nevaluation_pipeline.add_metric(ssim_metric)\n\n# Add LPIPS Metric to the evaluation pipeline\nlpips_metric = LPIPSMetric(image_size=evaluation_pipeline.image_size)\nevaluation_pipeline.add_metric(lpips_metric)\n\n# Evaluate!\nevaluation_pipeline(dataset=\"COCO:v0\")\n</code></pre> The evaluation pipeline will take each example, pass it through your application and score the output on multiple custom scoring functions using Weave Evaluation. By doing this, you'll have a view of the performance of your model, and a rich UI to drill into individual ouputs and scores."},{"location":"eval_pipelines/","title":"Evaluation Pipelines","text":"<p>Hemm evaluation pipelines for Diffusers pipelines.</p>"},{"location":"eval_pipelines/#hemm.eval_pipelines.BaseDiffusionModel","title":"<code>BaseDiffusionModel</code>","text":"<p>             Bases: <code>Model</code></p> <p>Base <code>weave.Model</code> wrapping <code>diffusers.DiffusionPipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>diffusion_model_name_or_path</code> <code>str</code> <p>The name or path of the diffusion model.</p> required <code>enable_cpu_offfload</code> <code>bool</code> <p>Enable CPU offload for the diffusion model.</p> required <code>image_height</code> <code>int</code> <p>The height of the generated image.</p> required <code>image_width</code> <code>int</code> <p>The width of the generated image.</p> required Source code in <code>hemm/eval_pipelines/model.py</code> <pre><code>class BaseDiffusionModel(weave.Model):\n    \"\"\"Base `weave.Model` wrapping `diffusers.DiffusionPipeline`.\n\n    Args:\n        diffusion_model_name_or_path (str): The name or path of the diffusion model.\n        enable_cpu_offfload (bool): Enable CPU offload for the diffusion model.\n        image_height (int): The height of the generated image.\n        image_width (int): The width of the generated image.\n    \"\"\"\n\n    diffusion_model_name_or_path: str\n    enable_cpu_offfload: bool = False\n    image_height: int = 512\n    image_width: int = 512\n    _torch_dtype: torch.dtype = torch.float16\n    _pipeline: DiffusionPipeline = None\n\n    def initialize(self):\n        self._pipeline = DiffusionPipeline.from_pretrained(\n            self.diffusion_model_name_or_path, torch_dtype=self._torch_dtype\n        )\n        if self.enable_cpu_offfload:\n            self._pipeline.enable_model_cpu_offload()\n        else:\n            self._pipeline = self._pipeline.to(\"cuda\")\n        self._pipeline.set_progress_bar_config(leave=False, desc=\"Generating Image\")\n\n    @weave.op()\n    def predict(self, prompt: str, seed: int) -&gt; Dict[str, str]:\n        image = self._pipeline(\n            prompt,\n            num_images_per_prompt=1,\n            height=self.image_height,\n            width=self.image_width,\n            generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n        ).images[0]\n        return {\"image\": base64_encode_image(image)}\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline","title":"<code>EvaluationPipeline</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Evaluation pipeline to evaluate the a multi-modal generative model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseDiffusionModel</code> <p>The model to evaluate.</p> required <code>seed</code> <code>int</code> <p>Seed value for the random number generator.</p> <code>42</code> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>class EvaluationPipeline(ABC):\n    \"\"\"Evaluation pipeline to evaluate the a multi-modal generative model.\n\n    Args:\n        model (BaseDiffusionModel): The model to evaluate.\n        seed (int): Seed value for the random number generator.\n    \"\"\"\n\n    def __init__(self, model: BaseDiffusionModel, seed: int = 42) -&gt; None:\n        super().__init__()\n        self.model = model\n        self.model.initialize()\n\n        self.image_size = (self.model.image_height, self.model.image_width)\n        self.seed = seed\n\n        self.inference_counter = 1\n        self.table_columns = [\"model\", \"prompt\", \"generated_image\"]\n        self.table_rows: List = []\n        self.wandb_table: wandb.Table = None\n        self.metric_functions: List[Callable] = []\n\n        self.evaluation_configs = {\n            \"pretrained_model_name_or_path\": self.model.diffusion_model_name_or_path,\n            \"torch_dtype\": str(self.model._torch_dtype),\n            \"enable_cpu_offfload\": self.model.enable_cpu_offfload,\n            \"image_size\": {\n                \"height\": self.image_size[0],\n                \"width\": self.image_size[1],\n            },\n            \"seed\": seed,\n            \"diffusion_pipeline\": dict(self.model._pipeline.config),\n        }\n\n    def add_metric(self, metric_fn: Callable):\n        \"\"\"Add a metric function to the evaluation pipeline.\n\n        Args:\n            metric_fn (Callable): Metric function to evaluate the generated images.\n        \"\"\"\n        self.table_columns.append(metric_fn.__class__.__name__)\n        self.evaluation_configs.update(metric_fn.config)\n        self.metric_functions.append(metric_fn)\n\n    @weave.op()\n    async def infer(self, prompt: str) -&gt; Dict[str, str]:\n        \"\"\"Inference function to generate images for the given prompt.\n\n        Args:\n            prompt (str): Prompt to generate the image.\n\n        Returns:\n            Dict[str, str]: Dictionary containing base64 encoded image to be logged as\n                a Weave object.\n        \"\"\"\n        if self.inference_counter == 1:\n            self.wandb_table = wandb.Table(columns=self.table_columns)\n        self.inference_counter += 1\n        output = self.model.predict(prompt, seed=self.seed)\n        self.table_rows.append(\n            [\n                self.model.diffusion_model_name_or_path,\n                prompt,\n                wandb.Image(\n                    Image.open(\n                        BytesIO(base64.b64decode(output[\"image\"].split(\";base64,\")[-1]))\n                    )\n                ),\n            ]\n        )\n        return output\n\n    def log_summary(self):\n        \"\"\"Log the evaluation summary to the Weights &amp; Biases dashboard.\"\"\"\n        config = wandb.config\n        config.update(self.evaluation_configs)\n        for row_idx, row in enumerate(self.table_rows):\n            current_row = row\n            for metric_fn in self.metric_functions:\n                current_row.append(metric_fn.scores[row_idx])\n            self.wandb_table.add_data(*current_row)\n        wandb.log(\n            {f\"Evalution/{self.model.diffusion_model_name_or_path}\": self.wandb_table}\n        )\n\n    def __call__(self, dataset: Union[List[Dict], str]) -&gt; None:\n        \"\"\"Evaluate the Stable Diffusion model on the given dataset.\n\n        Args:\n            dataset (Union[List[Dict], str]): Dataset to evaluate the model on. If a string is\n                passed, it is assumed to be a Weave dataset reference.\n        \"\"\"\n        dataset = weave.ref(dataset).get() if isinstance(dataset, str) else dataset\n        evaluation = Evaluation(\n            dataset=dataset,\n            scorers=[metric_fn.__call__ for metric_fn in self.metric_functions],\n        )\n        with weave.attributes(self.evaluation_configs):\n            asyncio.run(evaluation.evaluate(self.infer))\n        self.log_summary()\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.__call__","title":"<code>__call__(dataset)</code>","text":"<p>Evaluate the Stable Diffusion model on the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Union[List[Dict], str]</code> <p>Dataset to evaluate the model on. If a string is passed, it is assumed to be a Weave dataset reference.</p> required Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>def __call__(self, dataset: Union[List[Dict], str]) -&gt; None:\n    \"\"\"Evaluate the Stable Diffusion model on the given dataset.\n\n    Args:\n        dataset (Union[List[Dict], str]): Dataset to evaluate the model on. If a string is\n            passed, it is assumed to be a Weave dataset reference.\n    \"\"\"\n    dataset = weave.ref(dataset).get() if isinstance(dataset, str) else dataset\n    evaluation = Evaluation(\n        dataset=dataset,\n        scorers=[metric_fn.__call__ for metric_fn in self.metric_functions],\n    )\n    with weave.attributes(self.evaluation_configs):\n        asyncio.run(evaluation.evaluate(self.infer))\n    self.log_summary()\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.add_metric","title":"<code>add_metric(metric_fn)</code>","text":"<p>Add a metric function to the evaluation pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>metric_fn</code> <code>Callable</code> <p>Metric function to evaluate the generated images.</p> required Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>def add_metric(self, metric_fn: Callable):\n    \"\"\"Add a metric function to the evaluation pipeline.\n\n    Args:\n        metric_fn (Callable): Metric function to evaluate the generated images.\n    \"\"\"\n    self.table_columns.append(metric_fn.__class__.__name__)\n    self.evaluation_configs.update(metric_fn.config)\n    self.metric_functions.append(metric_fn)\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.infer","title":"<code>infer(prompt)</code>  <code>async</code>","text":"<p>Inference function to generate images for the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to generate the image.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dictionary containing base64 encoded image to be logged as a Weave object.</p> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>@weave.op()\nasync def infer(self, prompt: str) -&gt; Dict[str, str]:\n    \"\"\"Inference function to generate images for the given prompt.\n\n    Args:\n        prompt (str): Prompt to generate the image.\n\n    Returns:\n        Dict[str, str]: Dictionary containing base64 encoded image to be logged as\n            a Weave object.\n    \"\"\"\n    if self.inference_counter == 1:\n        self.wandb_table = wandb.Table(columns=self.table_columns)\n    self.inference_counter += 1\n    output = self.model.predict(prompt, seed=self.seed)\n    self.table_rows.append(\n        [\n            self.model.diffusion_model_name_or_path,\n            prompt,\n            wandb.Image(\n                Image.open(\n                    BytesIO(base64.b64decode(output[\"image\"].split(\";base64,\")[-1]))\n                )\n            ),\n        ]\n    )\n    return output\n</code></pre>"},{"location":"eval_pipelines/#hemm.eval_pipelines.EvaluationPipeline.log_summary","title":"<code>log_summary()</code>","text":"<p>Log the evaluation summary to the Weights &amp; Biases dashboard.</p> Source code in <code>hemm/eval_pipelines/eval_pipeline.py</code> <pre><code>def log_summary(self):\n    \"\"\"Log the evaluation summary to the Weights &amp; Biases dashboard.\"\"\"\n    config = wandb.config\n    config.update(self.evaluation_configs)\n    for row_idx, row in enumerate(self.table_rows):\n        current_row = row\n        for metric_fn in self.metric_functions:\n            current_row.append(metric_fn.scores[row_idx])\n        self.wandb_table.add_data(*current_row)\n    wandb.log(\n        {f\"Evalution/{self.model.diffusion_model_name_or_path}\": self.wandb_table}\n    )\n</code></pre>"},{"location":"utils/","title":"Hemm utilities","text":""},{"location":"utils/#hemm.utils.base64_decode_image","title":"<code>base64_decode_image(image)</code>","text":"<p>Decodes a base64 encoded image string encoded using the function <code>hemm.utils.base64_encode_image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image string encoded using the function <code>hemm.utils.base64_encode_image</code>.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: PIL Image object.</p> Source code in <code>hemm/utils.py</code> <pre><code>def base64_decode_image(image: str) -&gt; Image.Image:\n    \"\"\"Decodes a base64 encoded image string encoded using the function `hemm.utils.base64_encode_image`.\n\n    Args:\n        image (str): Base64 encoded image string encoded using the function `hemm.utils.base64_encode_image`.\n\n    Returns:\n        Image.Image: PIL Image object.\n    \"\"\"\n    return Image.open(io.BytesIO(base64.b64decode(image.split(\";base64,\")[-1])))\n</code></pre>"},{"location":"utils/#hemm.utils.base64_encode_image","title":"<code>base64_encode_image(image_path, mimetype=None)</code>","text":"<p>Converts an image to base64 encoded string to be logged and rendered on Weave dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Image]</code> <p>Path to the image or PIL Image object.</p> required <code>mimetype</code> <code>Optional[str]</code> <p>Mimetype of the image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Base64 encoded image string.</p> Source code in <code>hemm/utils.py</code> <pre><code>def base64_encode_image(\n    image_path: Union[str, Image.Image], mimetype: Optional[str] = None\n) -&gt; str:\n    \"\"\"Converts an image to base64 encoded string to be logged and rendered on Weave dashboard.\n\n    Args:\n        image_path (Union[str, Image.Image]): Path to the image or PIL Image object.\n        mimetype (Optional[str], optional): Mimetype of the image. Defaults to None.\n\n    Returns:\n        str: Base64 encoded image string.\n    \"\"\"\n    image = Image.open(image_path) if isinstance(image_path, str) else image_path\n    mimetype = (\n        EXT_TO_MIMETYPE[Path(image_path).suffix]\n        if isinstance(image_path, str)\n        else \"image/png\"\n    )\n    byte_arr = io.BytesIO()\n    image.save(byte_arr, format=\"PNG\")\n    encoded_string = base64.b64encode(byte_arr.getvalue()).decode(\"utf-8\")\n    encoded_string = f\"data:{mimetype};base64,{encoded_string}\"\n    return str(encoded_string)\n</code></pre>"},{"location":"utils/#hemm.utils.publish_dataset_to_weave","title":"<code>publish_dataset_to_weave(dataset_path, dataset_name=None, prompt_column=None, ground_truth_image_column=None, split=None, data_limit=None, get_weave_dataset_reference=True, dataset_transforms=None, column_transforms=None, dump_dir='./dump', *args, **kwargs)</code>","text":"<p>Publishes a HuggingFace dataset dictionary dataset as a Weave dataset.</p> Publish a subset of MSCOCO from Huggingface as a Weave Dataset <pre><code>import weave\nfrom hemm.utils import publish_dataset_to_weave\n\nif __name__ == \"__main__\":\n    weave.init(project_name=\"t2i_eval\")\n\n    dataset_reference = publish_dataset_to_weave(\n        dataset_path=\"HuggingFaceM4/COCO\",\n        prompt_column=\"sentences\",\n        ground_truth_image_column=\"image\",\n        split=\"validation\",\n        dataset_transforms=[\n            lambda item: {**item, \"sentences\": item[\"sentences\"][\"raw\"]}\n        ],\n        data_limit=5,\n    )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset_path</code> <code>[type]</code> <p>Path to the HuggingFace dataset.</p> required <code>dataset_name</code> <code>Optional[str]</code> <p>Name of the Weave dataset.</p> <code>None</code> <code>prompt_column</code> <code>Optional[str]</code> <p>Column name for prompt.</p> <code>None</code> <code>ground_truth_image_column</code> <code>Optional[str]</code> <p>Column name for ground truth image.</p> <code>None</code> <code>split</code> <code>Optional[str]</code> <p>Split to be used.</p> <code>None</code> <code>data_limit</code> <code>Optional[int]</code> <p>Limit the number of data items.</p> <code>None</code> <code>get_weave_dataset_reference</code> <code>bool</code> <p>Whether to return the Weave dataset reference.</p> <code>True</code> <code>dataset_transforms</code> <code>Optional[List[Callable]]</code> <p>List of dataset transforms.</p> <code>None</code> <code>column_transforms</code> <code>Optional[Dict[str, Callable]]</code> <p>Column specific transforms.</p> <code>None</code> <code>dump_dir</code> <code>Optional[str]</code> <p>Directory to dump the results.</p> <code>'./dump'</code> <p>Returns:</p> Type Description <code>Union[ObjectRef, None]</code> <p>Union[ObjectRef, None]: Weave dataset reference if get_weave_dataset_reference is True.</p> Source code in <code>hemm/utils.py</code> <pre><code>def publish_dataset_to_weave(\n    dataset_path,\n    dataset_name: Optional[str] = None,\n    prompt_column: Optional[str] = None,\n    ground_truth_image_column: Optional[str] = None,\n    split: Optional[str] = None,\n    data_limit: Optional[int] = None,\n    get_weave_dataset_reference: bool = True,\n    dataset_transforms: Optional[List[Callable]] = None,\n    column_transforms: Optional[Dict[str, Callable]] = None,\n    dump_dir: Optional[str] = \"./dump\",\n    *args,\n    **kwargs,\n) -&gt; Union[ObjectRef, None]:\n    \"\"\"Publishes a HuggingFace dataset dictionary dataset as a Weave dataset.\n\n    ??? example \"Publish a subset of MSCOCO from Huggingface as a Weave Dataset\"\n        ```python\n        import weave\n        from hemm.utils import publish_dataset_to_weave\n\n        if __name__ == \"__main__\":\n            weave.init(project_name=\"t2i_eval\")\n\n            dataset_reference = publish_dataset_to_weave(\n                dataset_path=\"HuggingFaceM4/COCO\",\n                prompt_column=\"sentences\",\n                ground_truth_image_column=\"image\",\n                split=\"validation\",\n                dataset_transforms=[\n                    lambda item: {**item, \"sentences\": item[\"sentences\"][\"raw\"]}\n                ],\n                data_limit=5,\n            )\n        ```\n\n    Args:\n        dataset_path ([type]): Path to the HuggingFace dataset.\n        dataset_name (Optional[str], optional): Name of the Weave dataset.\n        prompt_column (Optional[str], optional): Column name for prompt.\n        ground_truth_image_column (Optional[str], optional): Column name for ground truth image.\n        split (Optional[str], optional): Split to be used.\n        data_limit (Optional[int], optional): Limit the number of data items.\n        get_weave_dataset_reference (bool, optional): Whether to return the Weave dataset reference.\n        dataset_transforms (Optional[List[Callable]], optional): List of dataset transforms.\n        column_transforms (Optional[Dict[str, Callable]], optional): Column specific transforms.\n        dump_dir (Optional[str], optional): Directory to dump the results.\n\n    Returns:\n        Union[ObjectRef, None]: Weave dataset reference if get_weave_dataset_reference is True.\n    \"\"\"\n    os.makedirs(dump_dir, exist_ok=True)\n    dataset_name = dataset_name or Path(dataset_path).stem\n    dataset_dict = load_dataset(dataset_path, *args, **kwargs)\n    dataset_dict = dataset_dict[split] if split else dataset_dict[\"train\"]\n    dataset_dict = (\n        dataset_dict.select(range(data_limit))\n        if data_limit is not None and data_limit &lt; len(dataset_dict)\n        else dataset_dict\n    )\n    if dataset_transforms:\n        for transform in dataset_transforms:\n            dataset_dict = dataset_dict.map(transform)\n    dataset_dict = (\n        dataset_dict.rename_column(prompt_column, \"prompt\")\n        if prompt_column\n        else dataset_dict\n    )\n    dataset_dict = (\n        dataset_dict.rename_column(ground_truth_image_column, \"ground_truth_image\")\n        if ground_truth_image_column\n        else dataset_dict\n    )\n    column_transforms = (\n        {**column_transforms, **{\"ground_truth_image\": base64_encode_image}}\n        if column_transforms\n        else {\"ground_truth_image\": base64_encode_image}\n    )\n    weave_dataset_rows = []\n    for data_item in tqdm(dataset_dict):\n        for key in data_item.keys():\n            if column_transforms and key in column_transforms:\n                data_item[key] = column_transforms[key](data_item[key])\n        weave_dataset_rows.append(data_item)\n\n    if wandb.run:\n        save_weave_dataset_rows_to_artifacts(weave_dataset_rows, dump_dir)\n\n    weave_dataset = weave.Dataset(name=dataset_name, rows=weave_dataset_rows)\n    weave.publish(weave_dataset)\n    return weave.ref(dataset_name).get() if get_weave_dataset_reference else None\n</code></pre>"},{"location":"utils/#hemm.utils.save_weave_dataset_rows_to_artifacts","title":"<code>save_weave_dataset_rows_to_artifacts(dataset_rows, dump_dir)</code>","text":"<p>Saves the dataset rows to W&amp;B artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_rows</code> <code>List[Dict]</code> <p>List of dataset rows.</p> required <code>dump_dir</code> <code>str</code> <p>Directory to dump the results.</p> required Source code in <code>hemm/utils.py</code> <pre><code>def save_weave_dataset_rows_to_artifacts(\n    dataset_rows: List[Dict], dump_dir: str\n) -&gt; None:\n    \"\"\"Saves the dataset rows to W&amp;B artifacts.\n\n    Args:\n        dataset_rows (List[Dict]): List of dataset rows.\n        dump_dir (str): Directory to dump the results.\n    \"\"\"\n    with jsonlines.open(os.path.join(dump_dir, \"spatial.jsonl\"), mode=\"w\") as writer:\n        writer.write(dataset_rows)\n    artifact = wandb.Artifact(name=\"t2i_compbench_spatial_prompts\", type=\"dataset\")\n    artifact.add_file(local_path=os.path.join(dump_dir, \"data.jsonl\"))\n    wandb.log_artifact(artifact)\n</code></pre>"},{"location":"metrics/image_quality/","title":"Image Quality Metrics","text":""},{"location":"metrics/image_quality/#hemm.metrics.image_quality.LPIPSMetric","title":"<code>LPIPSMetric</code>","text":"<p>             Bases: <code>BaseImageQualityMetric</code></p> <p>LPIPS Metric to compute the Learned Perceptual Image Patch Similarity (LPIPS) score between two images. LPIPS essentially computes the similarity between the activations of two image patches for some pre-defined network. This measure has been shown to match human perception well. A low LPIPS score means that image patches are perceptual similar.</p> <p>Parameters:</p> Name Type Description Default <code>lpips_net_type</code> <code>str</code> <p>The network type to use for computing LPIPS. One of \"alex\", \"vgg\", or \"squeeze\".</p> <code>'alex'</code> <code>image_size</code> <code>Tuple[int, int]</code> <p>The size to which images will be resized before computing LPIPS.</p> <code>(512, 512)</code> <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'alexnet_learned_perceptual_image_patch_similarity'</code> Source code in <code>hemm/metrics/image_quality/lpips.py</code> <pre><code>class LPIPSMetric(BaseImageQualityMetric):\n    \"\"\"LPIPS Metric to compute the Learned Perceptual Image Patch Similarity (LPIPS) score\n    between two images. LPIPS essentially computes the similarity between the activations of\n    two image patches for some pre-defined network. This measure has been shown to match\n    human perception well. A low LPIPS score means that image patches are perceptual similar.\n\n    Args:\n        lpips_net_type (str): The network type to use for computing LPIPS. One of \"alex\", \"vgg\",\n            or \"squeeze\".\n        image_size (Tuple[int, int]): The size to which images will be resized before computing\n            LPIPS.\n        name (str): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        lpips_net_type: Literal[\"alex\", \"vgg\", \"squeeze\"] = \"alex\",\n        image_size: Optional[Tuple[int, int]] = (512, 512),\n        name: str = \"alexnet_learned_perceptual_image_patch_similarity\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.image_size = image_size\n        self.lpips_metric = partial(\n            learned_perceptual_image_patch_similarity, net_type=lpips_net_type\n        )\n        self.config = {\"lpips_net_type\": lpips_net_type}\n\n    @weave.op()\n    def compute_metric(\n        self, ground_truth_pil_image: Image, generated_pil_image: Image, prompt: str\n    ) -&gt; ComputeMetricOutput:\n        ground_truth_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(ground_truth_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 2, 1)\n            .float()\n        )\n        generated_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(generated_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 2, 1)\n            .float()\n        )\n        ground_truth_image = (ground_truth_image / 127.5) - 1.0\n        generated_image = (generated_image / 127.5) - 1.0\n        return ComputeMetricOutput(\n            score=float(\n                self.lpips_metric(generated_image, ground_truth_image).detach()\n            ),\n            ground_truth_image=base64_encode_image(ground_truth_pil_image),\n        )\n\n    @weave.op()\n    async def __call__(\n        self, prompt: str, ground_truth_image: str, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"LPIPSMetric\"\n        return super().__call__(prompt, ground_truth_image, model_output)\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.PSNRMetric","title":"<code>PSNRMetric</code>","text":"<p>             Bases: <code>BaseImageQualityMetric</code></p> <p>PSNR Metric to compute the Peak Signal-to-Noise Ratio (PSNR) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>psnr_data_range</code> <code>Optional[Union[float, Tuple[float, float]]]</code> <p>The data range of the input image (min, max). If None, the data range is determined from the image data type.</p> <code>None</code> <code>psnr_base</code> <code>float</code> <p>The base of the logarithm in the PSNR formula.</p> <code>10.0</code> <code>image_size</code> <code>Tuple[int, int]</code> <p>The size to which images will be resized before computing PSNR.</p> <code>(512, 512)</code> <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'peak_signal_noise_ratio'</code> Source code in <code>hemm/metrics/image_quality/psnr.py</code> <pre><code>class PSNRMetric(BaseImageQualityMetric):\n    \"\"\"PSNR Metric to compute the Peak Signal-to-Noise Ratio (PSNR) between two images.\n\n    Args:\n        psnr_data_range (Optional[Union[float, Tuple[float, float]]]): The data range of the input\n            image (min, max). If None, the data range is determined from the image data type.\n        psnr_base (float): The base of the logarithm in the PSNR formula.\n        image_size (Tuple[int, int]): The size to which images will be resized before computing\n            PSNR.\n        name (str): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        psnr_data_range: Optional[Union[float, Tuple[float, float]]] = None,\n        psnr_base: float = 10.0,\n        image_size: Optional[Tuple[int, int]] = (512, 512),\n        name: str = \"peak_signal_noise_ratio\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.image_size = image_size\n        self.psnr_metric = partial(\n            peak_signal_noise_ratio, data_range=psnr_data_range, base=psnr_base\n        )\n        self.config = {\n            \"psnr_base\": psnr_base,\n            \"psnr_data_range\": psnr_data_range,\n            \"image_size\": image_size,\n        }\n\n    @weave.op()\n    def compute_metric(\n        self,\n        ground_truth_pil_image: Image.Image,\n        generated_pil_image: Image.Image,\n        prompt: str,\n    ) -&gt; ComputeMetricOutput:\n        ground_truth_image = torch.from_numpy(\n            np.expand_dims(\n                np.array(ground_truth_pil_image.resize(self.image_size)), axis=0\n            ).astype(np.uint8)\n        ).float()\n        generated_image = torch.from_numpy(\n            np.expand_dims(\n                np.array(generated_pil_image.resize(self.image_size)), axis=0\n            ).astype(np.uint8)\n        ).float()\n        return ComputeMetricOutput(\n            score=float(self.psnr_metric(generated_image, ground_truth_image).detach()),\n            ground_truth_image=base64_encode_image(ground_truth_pil_image),\n        )\n\n    @weave.op()\n    async def __call__(\n        self, prompt: str, ground_truth_image: str, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"PSNRMetric\"\n        return super().__call__(prompt, ground_truth_image, model_output)\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.SSIMMetric","title":"<code>SSIMMetric</code>","text":"<p>             Bases: <code>BaseImageQualityMetric</code></p> <p>SSIM Metric to compute the Structural Similarity Index Measure (SSIM) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>ssim_gaussian_kernel</code> <code>bool</code> <p>Whether to use a Gaussian kernel for SSIM computation.</p> <code>True</code> <code>ssim_sigma</code> <code>float</code> <p>The standard deviation of the Gaussian kernel.</p> <code>1.5</code> <code>ssim_kernel_size</code> <code>int</code> <p>The size of the Gaussian kernel.</p> <code>11</code> <code>ssim_data_range</code> <code>Optional[Union[float, Tuple[float, float]]]</code> <p>The data range of the input image (min, max). If None, the data range is determined from the image data type.</p> <code>None</code> <code>ssim_k1</code> <code>float</code> <p>The constant used to stabilize the SSIM numerator.</p> <code>0.01</code> <code>ssim_k2</code> <code>float</code> <p>The constant used to stabilize the SSIM denominator.</p> <code>0.03</code> <code>image_size</code> <code>Tuple[int, int]</code> <p>The size to which images will be resized before computing SSIM.</p> <code>(512, 512)</code> <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'structural_similarity_index_measure'</code> Source code in <code>hemm/metrics/image_quality/ssim.py</code> <pre><code>class SSIMMetric(BaseImageQualityMetric):\n    \"\"\"SSIM Metric to compute the\n    [Structural Similarity Index Measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity)\n    between two images.\n\n    Args:\n        ssim_gaussian_kernel (bool): Whether to use a Gaussian kernel for SSIM computation.\n        ssim_sigma (float): The standard deviation of the Gaussian kernel.\n        ssim_kernel_size (int): The size of the Gaussian kernel.\n        ssim_data_range (Optional[Union[float, Tuple[float, float]]]): The data range of the input\n            image (min, max). If None, the data range is determined from the image data type.\n        ssim_k1 (float): The constant used to stabilize the SSIM numerator.\n        ssim_k2 (float): The constant used to stabilize the SSIM denominator.\n        image_size (Tuple[int, int]): The size to which images will be resized before computing\n            SSIM.\n        name (str): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        ssim_gaussian_kernel: bool = True,\n        ssim_sigma: float = 1.5,\n        ssim_kernel_size: int = 11,\n        ssim_data_range: Union[float, Tuple[float, float], None] = None,\n        ssim_k1: float = 0.01,\n        ssim_k2: float = 0.03,\n        image_size: Optional[Tuple[int, int]] = (512, 512),\n        name: str = \"structural_similarity_index_measure\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.image_size = image_size\n        self.ssim_metric = partial(\n            structural_similarity_index_measure,\n            gaussian_kernel=ssim_gaussian_kernel,\n            sigma=ssim_sigma,\n            kernel_size=ssim_kernel_size,\n            data_range=ssim_data_range,\n            k1=ssim_k1,\n            k2=ssim_k2,\n        )\n        self.config = {\n            \"ssim_gaussian_kernel\": ssim_gaussian_kernel,\n            \"ssim_sigma\": ssim_sigma,\n            \"ssim_kernel_size\": ssim_kernel_size,\n            \"ssim_data_range\": ssim_data_range,\n            \"ssim_k1\": ssim_k1,\n            \"ssim_k2\": ssim_k2,\n        }\n\n    @weave.op()\n    def compute_metric(\n        self,\n        ground_truth_pil_image: Image.Image,\n        generated_pil_image: Image.Image,\n        prompt: str,\n    ) -&gt; ComputeMetricOutput:\n        ground_truth_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(ground_truth_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 1, 2)\n            .float()\n        )\n        generated_image = (\n            torch.from_numpy(\n                np.expand_dims(\n                    np.array(generated_pil_image.resize(self.image_size)), axis=0\n                ).astype(np.uint8)\n            )\n            .permute(0, 3, 1, 2)\n            .float()\n        )\n        return ComputeMetricOutput(\n            score=float(self.ssim_metric(generated_image, ground_truth_image)),\n            ground_truth_image=base64_encode_image(ground_truth_pil_image),\n        )\n\n    @weave.op()\n    async def __call__(\n        self, prompt: str, ground_truth_image: str, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        _ = \"SSIMMetric\"\n        return super().__call__(prompt, ground_truth_image, model_output)\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric","title":"<code>BaseImageQualityMetric</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>class BaseImageQualityMetric(ABC):\n\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"Base class for Image Quality Metrics.\n\n        Args:\n            name (str): Name of the metric.\n        \"\"\"\n        super().__init__()\n        self.scores = []\n        self.name = name\n        self.config = {}\n\n    @abstractmethod\n    def compute_metric(\n        self,\n        ground_truth_pil_image: Image.Image,\n        generated_pil_image: Image.Image,\n        prompt: str,\n    ) -&gt; ComputeMetricOutput:\n        \"\"\"Compute the metric for the given images. This is an abstract\n        method and must be overriden by the child class implementation.\n\n        Args:\n            ground_truth_pil_image (Image.Image): Ground truth image in PIL format.\n            generated_pil_image (Image.Image): Generated image in PIL format.\n            prompt (str): Prompt for the image generation.\n\n        Returns:\n            ComputeMetricOutput: Output containing the metric score and ground truth image.\n        \"\"\"\n        pass\n\n    def __call__(\n        self, prompt: str, ground_truth_image: str, model_output: Dict[str, Any]\n    ) -&gt; Union[float, Dict[str, float]]:\n        \"\"\"Compute the metric for the given images. This method is used as the scorer\n        function for `weave.Evaluation` in the evaluation pipelines.\n\n        Args:\n            prompt (str): Prompt for the image generation.\n            ground_truth_image (str): Ground truth image in base64 format.\n            model_output (Dict[str, Any]): Model output containing the generated image.\n\n        Returns:\n            Union[float, Dict[str, float]]: Metric score.\n        \"\"\"\n        ground_truth_pil_image = Image.open(\n            BytesIO(base64.b64decode(ground_truth_image.split(\";base64,\")[-1]))\n        )\n        generated_pil_image = Image.open(\n            BytesIO(base64.b64decode(model_output[\"image\"].split(\";base64,\")[-1]))\n        )\n        metric_output = self.compute_metric(\n            ground_truth_pil_image, generated_pil_image, prompt\n        )\n        self.scores.append(metric_output.score)\n        return {self.name: metric_output.score}\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric.__call__","title":"<code>__call__(prompt, ground_truth_image, model_output)</code>","text":"<p>Compute the metric for the given images. This method is used as the scorer function for <code>weave.Evaluation</code> in the evaluation pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <code>ground_truth_image</code> <code>str</code> <p>Ground truth image in base64 format.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>Model output containing the generated image.</p> required <p>Returns:</p> Type Description <code>Union[float, Dict[str, float]]</code> <p>Union[float, Dict[str, float]]: Metric score.</p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>def __call__(\n    self, prompt: str, ground_truth_image: str, model_output: Dict[str, Any]\n) -&gt; Union[float, Dict[str, float]]:\n    \"\"\"Compute the metric for the given images. This method is used as the scorer\n    function for `weave.Evaluation` in the evaluation pipelines.\n\n    Args:\n        prompt (str): Prompt for the image generation.\n        ground_truth_image (str): Ground truth image in base64 format.\n        model_output (Dict[str, Any]): Model output containing the generated image.\n\n    Returns:\n        Union[float, Dict[str, float]]: Metric score.\n    \"\"\"\n    ground_truth_pil_image = Image.open(\n        BytesIO(base64.b64decode(ground_truth_image.split(\";base64,\")[-1]))\n    )\n    generated_pil_image = Image.open(\n        BytesIO(base64.b64decode(model_output[\"image\"].split(\";base64,\")[-1]))\n    )\n    metric_output = self.compute_metric(\n        ground_truth_pil_image, generated_pil_image, prompt\n    )\n    self.scores.append(metric_output.score)\n    return {self.name: metric_output.score}\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric.__init__","title":"<code>__init__(name)</code>","text":"<p>Base class for Image Quality Metrics.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"Base class for Image Quality Metrics.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n    super().__init__()\n    self.scores = []\n    self.name = name\n    self.config = {}\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.BaseImageQualityMetric.compute_metric","title":"<code>compute_metric(ground_truth_pil_image, generated_pil_image, prompt)</code>  <code>abstractmethod</code>","text":"<p>Compute the metric for the given images. This is an abstract method and must be overriden by the child class implementation.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_pil_image</code> <code>Image</code> <p>Ground truth image in PIL format.</p> required <code>generated_pil_image</code> <code>Image</code> <p>Generated image in PIL format.</p> required <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <p>Returns:</p> Name Type Description <code>ComputeMetricOutput</code> <code>ComputeMetricOutput</code> <p>Output containing the metric score and ground truth image.</p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>@abstractmethod\ndef compute_metric(\n    self,\n    ground_truth_pil_image: Image.Image,\n    generated_pil_image: Image.Image,\n    prompt: str,\n) -&gt; ComputeMetricOutput:\n    \"\"\"Compute the metric for the given images. This is an abstract\n    method and must be overriden by the child class implementation.\n\n    Args:\n        ground_truth_pil_image (Image.Image): Ground truth image in PIL format.\n        generated_pil_image (Image.Image): Generated image in PIL format.\n        prompt (str): Prompt for the image generation.\n\n    Returns:\n        ComputeMetricOutput: Output containing the metric score and ground truth image.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"metrics/image_quality/#hemm.metrics.image_quality.base.ComputeMetricOutput","title":"<code>ComputeMetricOutput</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Output of the metric computation function.</p> Source code in <code>hemm/metrics/image_quality/base.py</code> <pre><code>class ComputeMetricOutput(BaseModel):\n    \"\"\"Output of the metric computation function.\"\"\"\n\n    score: Union[float, Dict[str, float]]\n    ground_truth_image: str\n</code></pre>"},{"location":"metrics/prompt_image_alignment/","title":"Prompt-image Alignment Metrics","text":""},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.CLIPImageQualityScoreMetric","title":"<code>CLIPImageQualityScoreMetric</code>","text":"<p>             Bases: <code>BasePromptAlignmentMetric</code></p> <p>CLIP Image Quality Assessment metric for to measuring the visual content of images.</p> <p>The metric is based on the CLIP model, which is a neural network trained on a variety of (image, text) pairs to be able to generate a vector representation of the image and the text that is similar if the image and text are semantically similar.</p> <p>The metric works by calculating the cosine similarity between user provided images and pre-defined prompts. The prompts always comes in pairs of \u201cpositive\u201d and \u201cnegative\u201d such as \u201cGood photo.\u201d and \u201cBad photo.\u201d. By calculating the similartity between image embeddings and both the \u201cpositive\u201d and \u201cnegative\u201d prompt, the metric can determine which prompt the image is more similar to. The metric then returns the probability that the image is more similar to the first prompt than the second prompt.</p> <p>Parameters:</p> Name Type Description Default <code>clip_model_name_or_path</code> <code>str</code> <p>The name or path of the CLIP model to use. Defaults to \"clip_iqa\".</p> <code>'clip_iqa'</code> <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"clip_image_quality_assessment\".</p> <code>'clip_image_quality_assessment'</code> Source code in <code>hemm/metrics/prompt_alignment/clip_iqa_score.py</code> <pre><code>class CLIPImageQualityScoreMetric(BasePromptAlignmentMetric):\n    \"\"\"[CLIP Image Quality Assessment](https://arxiv.org/abs/2207.12396) metric\n    for to measuring the visual content of images.\n\n    The metric is based on the [CLIP](https://arxiv.org/abs/2103.00020) model,\n    which is a neural network trained on a variety of (image, text) pairs to be\n    able to generate a vector representation of the image and the text that is\n    similar if the image and text are semantically similar.\n\n    The metric works by calculating the cosine similarity between user provided images\n    and pre-defined prompts. The prompts always comes in pairs of \u201cpositive\u201d and \u201cnegative\u201d\n    such as \u201cGood photo.\u201d and \u201cBad photo.\u201d. By calculating the similartity between image\n    embeddings and both the \u201cpositive\u201d and \u201cnegative\u201d prompt, the metric can determine which\n    prompt the image is more similar to. The metric then returns the probability that the\n    image is more similar to the first prompt than the second prompt.\n\n    Args:\n        clip_model_name_or_path (str, optional): The name or path of the CLIP model to use.\n            Defaults to \"clip_iqa\".\n        name (str, optional): Name of the metric. Defaults to \"clip_image_quality_assessment\".\n    \"\"\"\n\n    def __init__(\n        self,\n        clip_model_name_or_path: str = \"clip_iqa\",\n        name: str = \"clip_image_quality_assessment\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.clip_iqa_fn = partial(\n            clip_image_quality_assessment, model_name_or_path=clip_model_name_or_path\n        )\n        self.built_in_prompts = [\n            \"quality\",\n            \"brightness\",\n            \"noisiness\",\n            \"colorfullness\",\n            \"sharpness\",\n            \"contrast\",\n            \"complexity\",\n            \"natural\",\n            \"happy\",\n            \"scary\",\n            \"new\",\n            \"real\",\n            \"beautiful\",\n            \"lonely\",\n            \"relaxing\",\n        ]\n        self.config = {\"clip_model_name_or_path\": clip_model_name_or_path}\n\n    @weave.op()\n    def compute_metric(\n        self, pil_image: Image, prompt: str\n    ) -&gt; Union[float, Dict[str, float]]:\n        images = np.expand_dims(np.array(pil_image), axis=0).astype(np.uint8) / 255.0\n        score_dict = {}\n        for prompt in tqdm(\n            self.built_in_prompts, desc=\"Calculating IQA scores\", leave=False\n        ):\n            clip_iqa_score = float(\n                self.clip_iqa_fn(\n                    images=torch.from_numpy(images).permute(0, 3, 1, 2),\n                    prompts=tuple([prompt] * images.shape[0]),\n                ).detach()\n            )\n            score_dict[f\"{self.name}_{prompt}\"] = clip_iqa_score\n        return score_dict\n\n    @weave.op()\n    async def __call__(\n        self, prompt: str, model_output: Dict[str, Any]\n    ) -&gt; Dict[str, float]:\n        _ = \"CLIPImageQualityScoreMetric\"\n        return super().__call__(prompt, model_output)\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.CLIPScoreMetric","title":"<code>CLIPScoreMetric</code>","text":"<p>             Bases: <code>BasePromptAlignmentMetric</code></p> <p>CLIP score metric for text-to-image similarity. CLIP Score is a reference free metric that can be used to evaluate the correlation between a generated caption for an image and the actual content of the image. It has been found to be highly correlated with human judgement.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"clip_score\".</p> <code>'clip_score'</code> <code>clip_model_name_or_path</code> <code>str</code> <p>The name or path of the CLIP model to use. Defaults to \"openai/clip-vit-base-patch16\".</p> <code>'openai/clip-vit-base-patch16'</code> Source code in <code>hemm/metrics/prompt_alignment/clip_score.py</code> <pre><code>class CLIPScoreMetric(BasePromptAlignmentMetric):\n    \"\"\"[CLIP score](https://arxiv.org/abs/2104.08718) metric for text-to-image similarity.\n    CLIP Score is a reference free metric that can be used to evaluate the correlation between\n    a generated caption for an image and the actual content of the image. It has been found to\n    be highly correlated with human judgement.\n\n    Args:\n        name (str, optional): Name of the metric. Defaults to \"clip_score\".\n        clip_model_name_or_path (str, optional): The name or path of the CLIP model to use.\n            Defaults to \"openai/clip-vit-base-patch16\".\n    \"\"\"\n\n    def __init__(\n        self,\n        clip_model_name_or_path: str = \"openai/clip-vit-base-patch16\",\n        name: str = \"clip_score\",\n    ) -&gt; None:\n        super().__init__(name)\n        self.clip_score_fn = partial(\n            clip_score, model_name_or_path=clip_model_name_or_path\n        )\n        self.config = {\"clip_model_name_or_path\": clip_model_name_or_path}\n\n    @weave.op()\n    def compute_metric(\n        self, pil_image: Image.Image, prompt: str\n    ) -&gt; Union[float, Dict[str, float]]:\n        images = np.expand_dims(np.array(pil_image), axis=0)\n        return float(\n            self.clip_score_fn(\n                torch.from_numpy(images).permute(0, 3, 1, 2), prompt\n            ).detach()\n        )\n\n    @weave.op()\n    async def __call__(\n        self, prompt: str, model_output: Dict[str, Any]\n    ) -&gt; Dict[str, float]:\n        _ = \"CLIPScoreMetric\"\n        return super().__call__(prompt, model_output)\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.base.BasePromptAlignmentMetric","title":"<code>BasePromptAlignmentMetric</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for Prompt Alignment Metrics.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>hemm/metrics/prompt_alignment/base.py</code> <pre><code>class BasePromptAlignmentMetric(ABC):\n    \"\"\"Base class for Prompt Alignment Metrics.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        super().__init__()\n        self.scores = []\n        self.name = name\n        self.config = {}\n\n    @abstractmethod\n    def compute_metric(\n        self, pil_image: Image.Image, prompt: str\n    ) -&gt; Union[float, Dict[str, float]]:\n        \"\"\"Compute the metric for the given image. This is an abstract\n        method and must be overriden by the child class implementation.\n\n        Args:\n            pil_image (Image.Image): Image in PIL format.\n            prompt (str): Prompt for the image generation.\n\n        Returns:\n            Union[float, Dict[str, float]]: Metric score.\n        \"\"\"\n        pass\n\n    def __call__(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, float]:\n        \"\"\"Compute the metric for the given image. This method is used as the scorer\n        function for `weave.Evaluation` in the evaluation pipelines.\n\n        Args:\n            prompt (str): Prompt for the image generation.\n            model_output (Dict[str, Any]): Model output containing the generated image.\n\n        Returns:\n            Dict[str, float]: Metric score.\n        \"\"\"\n        pil_image = Image.open(\n            BytesIO(base64.b64decode(model_output[\"image\"].split(\";base64,\")[-1]))\n        )\n        score = self.compute_metric(pil_image, prompt)\n        self.scores.append(score)\n        return {self.name: score}\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.base.BasePromptAlignmentMetric.__call__","title":"<code>__call__(prompt, model_output)</code>","text":"<p>Compute the metric for the given image. This method is used as the scorer function for <code>weave.Evaluation</code> in the evaluation pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>Model output containing the generated image.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: Metric score.</p> Source code in <code>hemm/metrics/prompt_alignment/base.py</code> <pre><code>def __call__(self, prompt: str, model_output: Dict[str, Any]) -&gt; Dict[str, float]:\n    \"\"\"Compute the metric for the given image. This method is used as the scorer\n    function for `weave.Evaluation` in the evaluation pipelines.\n\n    Args:\n        prompt (str): Prompt for the image generation.\n        model_output (Dict[str, Any]): Model output containing the generated image.\n\n    Returns:\n        Dict[str, float]: Metric score.\n    \"\"\"\n    pil_image = Image.open(\n        BytesIO(base64.b64decode(model_output[\"image\"].split(\";base64,\")[-1]))\n    )\n    score = self.compute_metric(pil_image, prompt)\n    self.scores.append(score)\n    return {self.name: score}\n</code></pre>"},{"location":"metrics/prompt_image_alignment/#hemm.metrics.prompt_alignment.base.BasePromptAlignmentMetric.compute_metric","title":"<code>compute_metric(pil_image, prompt)</code>  <code>abstractmethod</code>","text":"<p>Compute the metric for the given image. This is an abstract method and must be overriden by the child class implementation.</p> <p>Parameters:</p> Name Type Description Default <code>pil_image</code> <code>Image</code> <p>Image in PIL format.</p> required <code>prompt</code> <code>str</code> <p>Prompt for the image generation.</p> required <p>Returns:</p> Type Description <code>Union[float, Dict[str, float]]</code> <p>Union[float, Dict[str, float]]: Metric score.</p> Source code in <code>hemm/metrics/prompt_alignment/base.py</code> <pre><code>@abstractmethod\ndef compute_metric(\n    self, pil_image: Image.Image, prompt: str\n) -&gt; Union[float, Dict[str, float]]:\n    \"\"\"Compute the metric for the given image. This is an abstract\n    method and must be overriden by the child class implementation.\n\n    Args:\n        pil_image (Image.Image): Image in PIL format.\n        prompt (str): Prompt for the image generation.\n\n    Returns:\n        Union[float, Dict[str, float]]: Metric score.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"metrics/spatial_relationship/","title":"Spatial Relationship Metrics","text":"<p>This module aims to implement the Spatial relationship metric described in section 3.2 of T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</p> Using an object-detection model for spatial relationship evaluation as proposed in T2I-CompBench Weave gives us a holistic view of the evaluations to drill into individual ouputs and scores. Example <pre><code>import wandb\nimport weave\n\nfrom hemm.eval_pipelines import BaseDiffusionModel, EvaluationPipeline\nfrom hemm.metrics.image_quality import LPIPSMetric, PSNRMetric, SSIMMetric\n\n# Initialize Weave and WandB\nwandb.init(project=\"image-quality-leaderboard\", job_type=\"evaluation\")\nweave.init(project_name=\"image-quality-leaderboard\")\n\n# Initialize the diffusion model to be evaluated as a `weave.Model` using `BaseWeaveModel`\nmodel = BaseDiffusionModel(diffusion_model_name_or_path=\"CompVis/stable-diffusion-v1-4\")\n\n# Add the model to the evaluation pipeline\nevaluation_pipeline = EvaluationPipeline(model=model)\n\n# Define the judge model for 2d spatial relationship metric\njudge = DETRSpatialRelationShipJudge(\n    model_address=detr_model_address, revision=detr_revision\n)\n\n# Add PSNR Metric to the evaluation pipeline\nmetric = SpatialRelationshipMetric2D(judge=judge, name=\"2d_spatial_relationship_score\")\nevaluation_pipeline.add_metric(metric)\n\n# Evaluate!\nevaluation_pipeline(dataset=\"t2i_compbench_spatial_prompts:v0\")\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.spatial_relationship_2d.SpatialRelationshipMetric2D","title":"<code>SpatialRelationshipMetric2D</code>","text":"<p>Spatial relationship metric for 2D images as proposed by Section 4.2 from the paper T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation.</p> <p>Parameters:</p> Name Type Description Default <code>judge</code> <code>Union[Model, DETRSpatialRelationShipJudge]</code> <p>The judge model to predict the bounding boxes from the generated image.</p> required <code>iou_threshold</code> <code>Optional[float]</code> <p>The IoU threshold for the spatial relationship.</p> <code>0.1</code> <code>distance_threshold</code> <code>Optional[float]</code> <p>The distance threshold for the spatial relationship.</p> <code>150</code> <code>name</code> <code>Optional[str]</code> <p>The name of the metric.</p> <code>'spatial_relationship_score'</code> Source code in <code>hemm/metrics/spatial_relationship/spatial_relationship_2d.py</code> <pre><code>class SpatialRelationshipMetric2D:\n    \"\"\"Spatial relationship metric for 2D images as proposed by Section 4.2 from the paper\n    [T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation](https://arxiv.org/pdf/2307.06350).\n\n    Args:\n        judge (Union[weave.Model, DETRSpatialRelationShipJudge]): The judge model to predict\n            the bounding boxes from the generated image.\n        iou_threshold (Optional[float], optional): The IoU threshold for the spatial relationship.\n        distance_threshold (Optional[float], optional): The distance threshold for the spatial relationship.\n        name (Optional[str], optional): The name of the metric.\n    \"\"\"\n\n    def __init__(\n        self,\n        judge: Union[weave.Model, DETRSpatialRelationShipJudge],\n        iou_threshold: Optional[float] = 0.1,\n        distance_threshold: Optional[float] = 150,\n        name: Optional[str] = \"spatial_relationship_score\",\n    ) -&gt; None:\n        self.judge = judge\n        self.judge._initialize_models()\n        self.iou_threshold = iou_threshold\n        self.distance_threshold = distance_threshold\n        self.name = name\n        self.scores = []\n        self.config = judge.model_dump()\n\n    @weave.op()\n    def compose_judgement(\n        self,\n        prompt: str,\n        image: str,\n        entity_1: str,\n        entity_2: str,\n        relationship: str,\n        boxes: List[BoundingBox],\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Compose the judgement based on the response and the predicted bounding boxes.\n\n        Args:\n            prompt (str): The prompt using which the image was generated.\n            image (str): The base64 encoded image.\n            entity_1 (str): First entity.\n            entity_2 (str): Second entity.\n            relationship (str): Relationship between the entities.\n            boxes (List[BoundingBox]): The predicted bounding boxes.\n\n        Returns:\n            Dict[str, Any]: The comprehensive spatial relationship judgement.\n        \"\"\"\n        _ = prompt\n\n        # Determine presence of entities in the judgement\n        judgement = {\n            \"entity_1_present\": False,\n            \"entity_2_present\": False,\n        }\n        entity_1_box: BoundingBox = None\n        entity_2_box: BoundingBox = None\n        annotated_image = image\n        for box in boxes:\n            if box.label == entity_1:\n                judgement[\"entity_1_present\"] = True\n                entity_1_box = box\n            elif box.label == entity_2:\n                judgement[\"entity_2_present\"] = True\n                entity_2_box = box\n            annotated_image = annotate_with_bounding_box(annotated_image, box)\n\n        judgement[\"score\"] = 0.0\n        # assign score based on the spatial relationship inferred from the judgement\n        if judgement[\"entity_1_present\"] and judgement[\"entity_2_present\"]:\n            center_distance_x = abs(\n                entity_1_box.box_coordinates_center.x\n                - entity_2_box.box_coordinates_center.x\n            )\n            center_distance_y = abs(\n                entity_1_box.box_coordinates_center.y\n                - entity_2_box.box_coordinates_center.y\n            )\n            iou = get_iou(entity_1_box, entity_2_box)\n            score = 0.0\n            if relationship in [\"near\", \"next to\", \"on side of\", \"side of\"]:\n                if (\n                    abs(center_distance_x) &lt; self.distance_threshold\n                    or abs(center_distance_y) &lt; self.distance_threshold\n                ):\n                    score = 1.0\n                else:\n                    score = self.distance_threshold / max(\n                        abs(center_distance_x), abs(center_distance_y)\n                    )\n            elif relationship == \"on the right of\":\n                if center_distance_x &lt; 0:\n                    if (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1.0\n                    elif (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n            elif relationship == \"on the left of\":\n                if center_distance_x &gt; 0:\n                    if (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1.0\n                    elif (\n                        abs(center_distance_x) &gt; abs(center_distance_y)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n                else:\n                    score = 0.0\n            elif relationship == \"on the bottom of\":\n                if center_distance_y &lt; 0:\n                    if (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1\n                    elif (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n            elif relationship == \"on the top of\":\n                if center_distance_y &gt; 0:\n                    if (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &lt; self.iou_threshold\n                    ):\n                        score = 1\n                    elif (\n                        abs(center_distance_y) &gt; abs(center_distance_x)\n                        and iou &gt;= self.iou_threshold\n                    ):\n                        score = self.iou_threshold / iou\n            judgement[\"score\"] = score\n\n        self.scores.append(\n            {\n                **judgement,\n                **{\n                    \"judge_annotated_image\": wandb.Image(\n                        base64_decode_image(annotated_image)\n                        if isinstance(annotated_image, str)\n                        else annotated_image\n                    )\n                },\n            }\n        )\n        return {\n            **judgement,\n            **{\n                \"judge_annotated_image\": (\n                    base64_encode_image(annotated_image)\n                    if isinstance(annotated_image, Image.Image)\n                    else annotated_image\n                )\n            },\n        }\n\n    @weave.op()\n    async def __call__(\n        self,\n        prompt: str,\n        entity_1: str,\n        entity_2: str,\n        relationship: str,\n        model_output: Dict[str, Any],\n    ) -&gt; Dict[str, Union[bool, float, int]]:\n        \"\"\"Calculate the spatial relationship score for the given prompt and model output.\n\n        Args:\n            prompt (str): The prompt for the model.\n            entity_1 (str): The first entity in the spatial relationship.\n            entity_2 (str): The second entity in the spatial relationship.\n            relationship (str): The spatial relationship between the two entities.\n            model_output (Dict[str, Any]): The output from the model.\n\n        Returns:\n            Dict[str, Union[bool, float, int]]: The comprehensive spatial relationship judgement.\n        \"\"\"\n        _ = prompt\n\n        image = model_output[\"image\"]\n        boxes: List[BoundingBox] = self.judge.predict(image)\n        judgement = self.compose_judgement(\n            prompt, image, entity_1, entity_2, relationship, boxes\n        )\n        return {self.name: judgement[\"score\"]}\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.spatial_relationship_2d.SpatialRelationshipMetric2D.__call__","title":"<code>__call__(prompt, entity_1, entity_2, relationship, model_output)</code>  <code>async</code>","text":"<p>Calculate the spatial relationship score for the given prompt and model output.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt for the model.</p> required <code>entity_1</code> <code>str</code> <p>The first entity in the spatial relationship.</p> required <code>entity_2</code> <code>str</code> <p>The second entity in the spatial relationship.</p> required <code>relationship</code> <code>str</code> <p>The spatial relationship between the two entities.</p> required <code>model_output</code> <code>Dict[str, Any]</code> <p>The output from the model.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[bool, float, int]]</code> <p>Dict[str, Union[bool, float, int]]: The comprehensive spatial relationship judgement.</p> Source code in <code>hemm/metrics/spatial_relationship/spatial_relationship_2d.py</code> <pre><code>@weave.op()\nasync def __call__(\n    self,\n    prompt: str,\n    entity_1: str,\n    entity_2: str,\n    relationship: str,\n    model_output: Dict[str, Any],\n) -&gt; Dict[str, Union[bool, float, int]]:\n    \"\"\"Calculate the spatial relationship score for the given prompt and model output.\n\n    Args:\n        prompt (str): The prompt for the model.\n        entity_1 (str): The first entity in the spatial relationship.\n        entity_2 (str): The second entity in the spatial relationship.\n        relationship (str): The spatial relationship between the two entities.\n        model_output (Dict[str, Any]): The output from the model.\n\n    Returns:\n        Dict[str, Union[bool, float, int]]: The comprehensive spatial relationship judgement.\n    \"\"\"\n    _ = prompt\n\n    image = model_output[\"image\"]\n    boxes: List[BoundingBox] = self.judge.predict(image)\n    judgement = self.compose_judgement(\n        prompt, image, entity_1, entity_2, relationship, boxes\n    )\n    return {self.name: judgement[\"score\"]}\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.spatial_relationship_2d.SpatialRelationshipMetric2D.compose_judgement","title":"<code>compose_judgement(prompt, image, entity_1, entity_2, relationship, boxes)</code>","text":"<p>Compose the judgement based on the response and the predicted bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt using which the image was generated.</p> required <code>image</code> <code>str</code> <p>The base64 encoded image.</p> required <code>entity_1</code> <code>str</code> <p>First entity.</p> required <code>entity_2</code> <code>str</code> <p>Second entity.</p> required <code>relationship</code> <code>str</code> <p>Relationship between the entities.</p> required <code>boxes</code> <code>List[BoundingBox]</code> <p>The predicted bounding boxes.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The comprehensive spatial relationship judgement.</p> Source code in <code>hemm/metrics/spatial_relationship/spatial_relationship_2d.py</code> <pre><code>@weave.op()\ndef compose_judgement(\n    self,\n    prompt: str,\n    image: str,\n    entity_1: str,\n    entity_2: str,\n    relationship: str,\n    boxes: List[BoundingBox],\n) -&gt; Dict[str, Any]:\n    \"\"\"Compose the judgement based on the response and the predicted bounding boxes.\n\n    Args:\n        prompt (str): The prompt using which the image was generated.\n        image (str): The base64 encoded image.\n        entity_1 (str): First entity.\n        entity_2 (str): Second entity.\n        relationship (str): Relationship between the entities.\n        boxes (List[BoundingBox]): The predicted bounding boxes.\n\n    Returns:\n        Dict[str, Any]: The comprehensive spatial relationship judgement.\n    \"\"\"\n    _ = prompt\n\n    # Determine presence of entities in the judgement\n    judgement = {\n        \"entity_1_present\": False,\n        \"entity_2_present\": False,\n    }\n    entity_1_box: BoundingBox = None\n    entity_2_box: BoundingBox = None\n    annotated_image = image\n    for box in boxes:\n        if box.label == entity_1:\n            judgement[\"entity_1_present\"] = True\n            entity_1_box = box\n        elif box.label == entity_2:\n            judgement[\"entity_2_present\"] = True\n            entity_2_box = box\n        annotated_image = annotate_with_bounding_box(annotated_image, box)\n\n    judgement[\"score\"] = 0.0\n    # assign score based on the spatial relationship inferred from the judgement\n    if judgement[\"entity_1_present\"] and judgement[\"entity_2_present\"]:\n        center_distance_x = abs(\n            entity_1_box.box_coordinates_center.x\n            - entity_2_box.box_coordinates_center.x\n        )\n        center_distance_y = abs(\n            entity_1_box.box_coordinates_center.y\n            - entity_2_box.box_coordinates_center.y\n        )\n        iou = get_iou(entity_1_box, entity_2_box)\n        score = 0.0\n        if relationship in [\"near\", \"next to\", \"on side of\", \"side of\"]:\n            if (\n                abs(center_distance_x) &lt; self.distance_threshold\n                or abs(center_distance_y) &lt; self.distance_threshold\n            ):\n                score = 1.0\n            else:\n                score = self.distance_threshold / max(\n                    abs(center_distance_x), abs(center_distance_y)\n                )\n        elif relationship == \"on the right of\":\n            if center_distance_x &lt; 0:\n                if (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1.0\n                elif (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n        elif relationship == \"on the left of\":\n            if center_distance_x &gt; 0:\n                if (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1.0\n                elif (\n                    abs(center_distance_x) &gt; abs(center_distance_y)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n            else:\n                score = 0.0\n        elif relationship == \"on the bottom of\":\n            if center_distance_y &lt; 0:\n                if (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1\n                elif (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n        elif relationship == \"on the top of\":\n            if center_distance_y &gt; 0:\n                if (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &lt; self.iou_threshold\n                ):\n                    score = 1\n                elif (\n                    abs(center_distance_y) &gt; abs(center_distance_x)\n                    and iou &gt;= self.iou_threshold\n                ):\n                    score = self.iou_threshold / iou\n        judgement[\"score\"] = score\n\n    self.scores.append(\n        {\n            **judgement,\n            **{\n                \"judge_annotated_image\": wandb.Image(\n                    base64_decode_image(annotated_image)\n                    if isinstance(annotated_image, str)\n                    else annotated_image\n                )\n            },\n        }\n    )\n    return {\n        **judgement,\n        **{\n            \"judge_annotated_image\": (\n                base64_encode_image(annotated_image)\n                if isinstance(annotated_image, Image.Image)\n                else annotated_image\n            )\n        },\n    }\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.judges.DETRSpatialRelationShipJudge","title":"<code>DETRSpatialRelationShipJudge</code>","text":"<p>             Bases: <code>Model</code></p> <p>DETR spatial relationship judge model for 2D images.</p> <p>Parameters:</p> Name Type Description Default <code>model_address</code> <code>str</code> <p>The address of the model to use.</p> required <code>revision</code> <code>str</code> <p>The revision of the model to use.</p> required Source code in <code>hemm/metrics/spatial_relationship/judges/detr.py</code> <pre><code>class DETRSpatialRelationShipJudge(weave.Model):\n    \"\"\"DETR spatial relationship judge model for 2D images.\n\n    Args:\n        model_address (str, optional): The address of the model to use.\n        revision (str, optional): The revision of the model to use.\n    \"\"\"\n\n    model_address: str = \"facebook/detr-resnet-50\"\n    revision: str = \"no_timm\"\n    _feature_extractor: DetrImageProcessor = None\n    _object_detection_model: DetrForObjectDetection = None\n\n    def _initialize_models(self):\n        self._feature_extractor = DetrImageProcessor.from_pretrained(\n            self.model_address, revision=self.revision\n        )\n        self._object_detection_model = DetrForObjectDetection.from_pretrained(\n            self.model_address, revision=self.revision\n        )\n\n    @weave.op()\n    def predict(self, image: str) -&gt; List[BoundingBox]:\n        \"\"\"Predict the bounding boxes from the input image.\n\n        Args:\n            image (str): The base64 encoded image.\n\n        Returns:\n            List[BoundingBox]: The predicted bounding boxes.\n        \"\"\"\n        pil_image = base64_decode_image(image)\n        encoding = self._feature_extractor(pil_image, return_tensors=\"pt\")\n        outputs = self._object_detection_model(**encoding)\n        target_sizes = torch.tensor([pil_image.size[::-1]])\n        results = self._feature_extractor.post_process_object_detection(\n            outputs, target_sizes=target_sizes, threshold=0.9\n        )[0]\n        bboxes = []\n        for score, label, box in zip(\n            results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n        ):\n            xmin, ymin, xmax, ymax = box.tolist()\n            bboxes.append(\n                BoundingBox(\n                    box_coordinates_min=CartesianCoordinate2D(x=xmin, y=ymin),\n                    box_coordinates_max=CartesianCoordinate2D(x=xmax, y=ymax),\n                    box_coordinates_center=CartesianCoordinate2D(\n                        x=(xmin + xmax) / 2, y=(ymin + ymax) / 2\n                    ),\n                    label=self._object_detection_model.config.id2label[label.item()],\n                    score=score.item(),\n                )\n            )\n        return bboxes\n</code></pre>"},{"location":"metrics/spatial_relationship/#hemm.metrics.spatial_relationship.judges.DETRSpatialRelationShipJudge.predict","title":"<code>predict(image)</code>","text":"<p>Predict the bounding boxes from the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The base64 encoded image.</p> required <p>Returns:</p> Type Description <code>List[BoundingBox]</code> <p>List[BoundingBox]: The predicted bounding boxes.</p> Source code in <code>hemm/metrics/spatial_relationship/judges/detr.py</code> <pre><code>@weave.op()\ndef predict(self, image: str) -&gt; List[BoundingBox]:\n    \"\"\"Predict the bounding boxes from the input image.\n\n    Args:\n        image (str): The base64 encoded image.\n\n    Returns:\n        List[BoundingBox]: The predicted bounding boxes.\n    \"\"\"\n    pil_image = base64_decode_image(image)\n    encoding = self._feature_extractor(pil_image, return_tensors=\"pt\")\n    outputs = self._object_detection_model(**encoding)\n    target_sizes = torch.tensor([pil_image.size[::-1]])\n    results = self._feature_extractor.post_process_object_detection(\n        outputs, target_sizes=target_sizes, threshold=0.9\n    )[0]\n    bboxes = []\n    for score, label, box in zip(\n        results[\"scores\"], results[\"labels\"], results[\"boxes\"]\n    ):\n        xmin, ymin, xmax, ymax = box.tolist()\n        bboxes.append(\n            BoundingBox(\n                box_coordinates_min=CartesianCoordinate2D(x=xmin, y=ymin),\n                box_coordinates_max=CartesianCoordinate2D(x=xmax, y=ymax),\n                box_coordinates_center=CartesianCoordinate2D(\n                    x=(xmin + xmax) / 2, y=(ymin + ymax) / 2\n                ),\n                label=self._object_detection_model.config.id2label[label.item()],\n                score=score.item(),\n            )\n        )\n    return bboxes\n</code></pre>"}]}